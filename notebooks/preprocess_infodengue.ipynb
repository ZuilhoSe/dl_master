{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gtda.time_series import SlidingWindow, TakensEmbedding\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "from gtda.diagrams import PersistenceEntropy, Amplitude\n",
    "from tqdm import tqdm\n",
    "import json"
   ],
   "id": "4959d0a08f88e752",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Opening data",
   "id": "b51182ec39c783f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "file_path = \"../data/raw/data_sprint_2025/dengue.csv\"\n",
    "data_casos = pd.read_csv(file_path)\n",
    "display(data_casos.head())\n",
    "print(f\"Columns: {data_casos.columns}\")"
   ],
   "id": "b662e5e872b4229c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "file_path = \"../data/raw/data_sprint_2025/climate.csv\"\n",
    "data_climate = pd.read_csv(file_path)\n",
    "display(data_climate.head())\n",
    "print(f\"Columns: {data_climate.columns}\")"
   ],
   "id": "48b7f9536929398d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "file_path = \"../data/raw/data_sprint_2025/environ_vars.csv\"\n",
    "data_environ = pd.read_csv(file_path)\n",
    "display(data_environ.head())\n",
    "print(f\"Columns: {data_environ.columns}\")"
   ],
   "id": "e837592efed5009c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "file_path = \"../data/raw/data_sprint_2025/forecasting_climate.csv\"\n",
    "data_forecast_climate = pd.read_csv(file_path)\n",
    "display(data_forecast_climate.head())\n",
    "print(f\"Columns: {data_forecast_climate.columns}\")"
   ],
   "id": "5f84063164f24af0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "file_path = \"../data/raw/data_sprint_2025/ocean_climate_oscillations.csv\"\n",
    "data_ocean = pd.read_csv(file_path)\n",
    "display(data_ocean.head())\n",
    "print(f\"Columns: {data_ocean.columns}\")"
   ],
   "id": "c4f11eddae41471c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "file_path = \"../data/raw/data_sprint_2025/datasus_population_2001_2024.csv\"\n",
    "data_pop = pd.read_csv(file_path)\n",
    "display(data_pop.head())\n",
    "print(f\"Columns: {data_pop.columns}\")"
   ],
   "id": "df00730087ec435a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "file_path = \"../data/raw/data_sprint_2025/map_regional_health.csv\"\n",
    "data_reg_health = pd.read_csv(file_path)\n",
    "display(data_reg_health.head())\n",
    "print(f\"Columns: {data_reg_health.columns}\")"
   ],
   "id": "aaaefc117a50f8c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "file_path = \"../data/raw/data_sprint_2025/dados_episcanner.csv\"\n",
    "data_episcanner = pd.read_csv(file_path)\n",
    "display(data_episcanner.head())\n",
    "print(f\"Columns: {data_episcanner.columns}\")"
   ],
   "id": "57ff4508f60287ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Pre-processing",
   "id": "e6b0320502777fe4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_rolling_tda(df, target_col='casos', group_col='geocode', window_size=53, stride=1):\n",
    "    \"\"\"\n",
    "    Compute topological features using a sliding window.\n",
    "    \"\"\"\n",
    "    print(f\"BEGGINING TDA EXTRACTION (Window={window_size}, Column='{target_col}')...\")\n",
    "\n",
    "    TE = TakensEmbedding(dimension=3, time_delay=1)\n",
    "    VR = VietorisRipsPersistence(homology_dimensions=[0, 1])\n",
    "    PE = PersistenceEntropy()\n",
    "    AMP = Amplitude(metric='wasserstein')\n",
    "\n",
    "    tda_results = []\n",
    "\n",
    "    unique_geocodes = df[group_col].unique()\n",
    "\n",
    "    for geo in tqdm(unique_geocodes, desc=\"Processando Cidades\"):\n",
    "        city_data = df[df[group_col] == geo].sort_values('time_idx')\n",
    "        series = city_data[target_col].values\n",
    "\n",
    "        if len(series) < window_size:\n",
    "            continue\n",
    "\n",
    "        SW = SlidingWindow(size=window_size, stride=stride)\n",
    "        windows = SW.fit_transform(series)\n",
    "\n",
    "        try:\n",
    "            point_clouds = TE.fit_transform(windows)\n",
    "\n",
    "            diagrams = VR.fit_transform(point_clouds)\n",
    "\n",
    "            entropy = PE.fit_transform(diagrams)\n",
    "            amplitude = AMP.fit_transform(diagrams)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro na cidade {geo}: {e}\")\n",
    "            continue\n",
    "\n",
    "        valid_indices = city_data.index[window_size - 1 :]\n",
    "\n",
    "        df_city_tda = pd.DataFrame(index=valid_indices)\n",
    "        df_city_tda[group_col] = geo\n",
    "\n",
    "        df_city_tda['tda_entropy_H0'] = entropy[:, 0]\n",
    "        df_city_tda['tda_entropy_H1'] = entropy[:, 1]\n",
    "        df_city_tda['tda_amplitude_H0'] = amplitude[:, 0]\n",
    "        df_city_tda['tda_amplitude_H1'] = amplitude[:, 1]\n",
    "\n",
    "        tda_results.append(df_city_tda)\n",
    "\n",
    "    if not tda_results:\n",
    "        print(\"No results found.\")\n",
    "        return df\n",
    "\n",
    "    df_tda_final = pd.concat(tda_results)\n",
    "\n",
    "    print(\"Merging on main df\")\n",
    "    df_merged = df.merge(\n",
    "        df_tda_final,\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        how='left',\n",
    "        suffixes=('', '_drop')\n",
    "    )\n",
    "\n",
    "    cols_to_drop = [c for c in df_merged.columns if '_drop' in c]\n",
    "    df_merged.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    tda_cols = ['tda_entropy_H0', 'tda_entropy_H1', 'tda_amplitude_H0', 'tda_amplitude_H1']\n",
    "    df_merged[tda_cols] = df_merged[tda_cols].fillna(0)\n",
    "\n",
    "    print(\"Finished\")\n",
    "    return df_merged"
   ],
   "id": "917ffb4962545bd0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def preprocess_for_tft(\n",
    "    df_casos,\n",
    "    df_climate,\n",
    "    df_environ,\n",
    "    df_forecast_climate,\n",
    "    df_ocean,\n",
    "    df_pop,\n",
    "    df_reg_health,\n",
    "    df_episcanner\n",
    "):\n",
    "    \"\"\"\n",
    "    Process all data for TFT training\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Beggining Pre-processing\")\n",
    "\n",
    "    df = df_casos.copy()\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df_climate['date'] = pd.to_datetime(df_climate['date'])\n",
    "    df_ocean['date'] = pd.to_datetime(df_ocean['date'])\n",
    "\n",
    "    df = df.sort_values(['geocode', 'date']).reset_index(drop=True)\n",
    "\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    min_date = df['date'].min()\n",
    "    df['time_idx'] = ((df['date'] - min_date).dt.days / 7).astype(int)\n",
    "\n",
    "    if 'epiweek' in df.columns:\n",
    "        df['week_of_year'] = df['epiweek'].astype(str).str[-2:].astype(int)\n",
    "    else:\n",
    "        df['week_of_year'] = df['date'].dt.isocalendar().week.astype(int)\n",
    "\n",
    "    df['week_cycle'] = df['week_of_year'].apply(lambda x: x - 40 if x >= 41 else x + 12)\n",
    "\n",
    "    df['sin_week_cycle'] = np.sin(2 * np.pi * df['week_cycle'] / 52)\n",
    "    df['cos_week_cycle'] = np.cos(2 * np.pi * df['week_cycle'] / 52)\n",
    "\n",
    "    print(\"Integrating Climate\")\n",
    "\n",
    "    cols_clima = [c for c in df_climate.columns if c not in ['epiweek']]\n",
    "    df = pd.merge(df, df_climate[cols_clima], on=['geocode', 'date'], how='left')\n",
    "\n",
    "    present_climate_cols = [c for c in df_climate.columns if c in df.columns and c not in ['geocode', 'date']]\n",
    "    df[present_climate_cols] = df.groupby('geocode')[present_climate_cols].ffill()\n",
    "\n",
    "    print(\"Integrating Ocean Climate\")\n",
    "\n",
    "    df = pd.merge(df, df_ocean, on='date', how='left')\n",
    "    df[['enso', 'iod', 'pdo']] = df[['enso', 'iod', 'pdo']].ffill()\n",
    "\n",
    "    print(\"Integrating Forecasting\")\n",
    "\n",
    "    df_fc = df_forecast_climate.copy()\n",
    "\n",
    "    df_fc['reference_month'] = pd.to_datetime(df_fc['reference_month'])\n",
    "\n",
    "    df_fc['valid_date'] = df_fc.apply(\n",
    "        lambda x: x['reference_month'] + pd.DateOffset(months=int(x['forecast_months_ahead'])),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    df_fc['year'] = df_fc['valid_date'].dt.year\n",
    "    df_fc['month'] = df_fc['valid_date'].dt.month\n",
    "\n",
    "    rename_dict = {\n",
    "        'temp_med': 'forecast_temp_med',\n",
    "        'umid_med': 'forecast_umid_med',\n",
    "        'precip_tot': 'forecast_precip_tot'\n",
    "    }\n",
    "    df_fc = df_fc.rename(columns=rename_dict)\n",
    "\n",
    "    cols_to_use = ['geocode', 'year', 'month', 'forecast_temp_med', 'forecast_umid_med', 'forecast_precip_tot']\n",
    "    df_fc_clean = df_fc[cols_to_use].groupby(['geocode', 'year', 'month']).mean().reset_index()\n",
    "\n",
    "    df = pd.merge(df, df_fc_clean, on=['geocode', 'year', 'month'], how='left')\n",
    "\n",
    "    for col in rename_dict.values():\n",
    "        df[col] = df[col].ffill()\n",
    "\n",
    "    print(\"Integrating Population\")\n",
    "    df = pd.merge(df, df_pop, on=['geocode', 'year'], how='left')\n",
    "\n",
    "    df['log_pop'] = np.log1p(df['population'])\n",
    "    df['log_pop'] = df.groupby('geocode')['log_pop'].ffill()\n",
    "\n",
    "    print(\"Integrating Environmental Variables\")\n",
    "\n",
    "    if 'uf_code' in df.columns and 'uf_code' in df_environ.columns:\n",
    "        df = df.drop(columns=['uf_code'])\n",
    "\n",
    "    df = pd.merge(df, df_environ, on='geocode', how='left')\n",
    "\n",
    "    cols_reg = ['geocode', 'macroregion_name', 'regional_name']\n",
    "    cols_reg_exist = [c for c in cols_reg if c in df_reg_health.columns]\n",
    "    df = pd.merge(df, df_reg_health[cols_reg_exist], on='geocode', how='left')\n",
    "\n",
    "    print(\"Integrating Episcanner Data\")\n",
    "\n",
    "    target_cols = ['geocode', 'year', 'R0', 'peak_week', 'total_cases', 'alpha', 'beta']\n",
    "    df_epi_targets = df_episcanner[target_cols].copy()\n",
    "\n",
    "    df_epi_targets['log_total_cases'] = np.log1p(df_epi_targets['total_cases'])\n",
    "\n",
    "    df = pd.merge(df, df_epi_targets, on=['geocode', 'year'], how='left')\n",
    "\n",
    "    print(f\"Null: {len(df)}\")\n",
    "    df = df.dropna(subset=['R0'])\n",
    "    print(f\"Complete: {len(df)}\")\n",
    "\n",
    "    df['casos'] = df['casos'].fillna(0)\n",
    "    df['incidence'] = (df['casos'] / df['population']) * 100000\n",
    "    df['incidence'] = df['incidence'].fillna(0)\n",
    "\n",
    "    print(\"Calculating TDA Features\")\n",
    "    df = calculate_rolling_tda(\n",
    "        df,\n",
    "        target_col='incidence',\n",
    "        group_col='geocode'\n",
    "    )\n",
    "\n",
    "    known_reals = [\n",
    "        \"time_idx\",\n",
    "        \"week_cycle\",\n",
    "        \"sin_week_cycle\",\n",
    "        \"cos_week_cycle\",\n",
    "        \"log_pop\",\n",
    "        \"forecast_temp_med\",\n",
    "        \"forecast_umid_med\",\n",
    "        \"forecast_precip_tot\"\n",
    "    ]\n",
    "\n",
    "    tda_features = ['tda_entropy_H0', 'tda_entropy_H1', 'tda_amplitude_H0', 'tda_amplitude_H1']\n",
    "\n",
    "    unknown_reals = [\n",
    "        \"casos\",\n",
    "        \"incidence\",\n",
    "        \"temp_med\",\n",
    "        \"precip_med\",\n",
    "        \"rel_humid_med\",\n",
    "        \"enso\",\n",
    "        \"iod\"\n",
    "    ] + tda_features\n",
    "\n",
    "    static_cats = [\"koppen\", \"biome\", \"macroregion_name\"]\n",
    "\n",
    "    targets = [\"R0\", \"peak_week\", \"log_total_cases\", \"alpha\", \"beta\"]\n",
    "\n",
    "    print(\"Finished!\")\n",
    "    return df, known_reals, unknown_reals, static_cats, targets"
   ],
   "id": "62267dc7b3b1f9bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_final, known, unknown, statics, targets = preprocess_for_tft(\n",
    "    data_casos, data_climate, data_environ, data_forecast_climate,\n",
    "    data_ocean, data_pop, data_reg_health, data_episcanner\n",
    ")"
   ],
   "id": "4037ec33929ad80a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Saving",
   "id": "3916635df62eab87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def save_optimized_dataset(df, filepath):\n",
    "    \"\"\"\n",
    "    Optimize dataset memory usage and save to parquet.\n",
    "    \"\"\"\n",
    "    float_cols = df.select_dtypes(include=['float64']).columns\n",
    "    df[float_cols] = df[float_cols].astype('float32')\n",
    "\n",
    "    int_cols = df.select_dtypes(include=['int64', 'int']).columns\n",
    "    for col in int_cols:\n",
    "        c_min = df[col].min()\n",
    "        c_max = df[col].max()\n",
    "\n",
    "        if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "            df[col] = df[col].astype(np.int8)\n",
    "        elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "            df[col] = df[col].astype(np.int16)\n",
    "        elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "            df[col] = df[col].astype(np.int32)\n",
    "\n",
    "    categorical_candidates = [\n",
    "        'uf', 'biome', 'koppen', 'macroregion_name', 'regional_name',\n",
    "        'month', 'geocode_name'\n",
    "    ]\n",
    "\n",
    "    for col in categorical_candidates:\n",
    "        if col in df.columns:\n",
    "            if df[col].nunique() / len(df) < 0.5:\n",
    "                df[col] = df[col].astype('category')\n",
    "\n",
    "    print(\"Saving compressed parquet\")\n",
    "    df.to_parquet(\n",
    "        filepath,\n",
    "        compression='zstd',\n",
    "        index=False,\n",
    "        engine='pyarrow'\n",
    "    )\n",
    "    print(f\"Saved to: {filepath}\")"
   ],
   "id": "dd364cdb7a777174",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def save_tft_config(known, unknown, statics, targets, filepath=\"../data/processed/tft_config.json\"):\n",
    "    \"\"\"\n",
    "    Save a JSON config file with the dataset's metadata for TFT.\n",
    "    \"\"\"\n",
    "    config = {\n",
    "        \"time_varying_known_reals\": known,\n",
    "        \"time_varying_unknown_reals\": unknown,\n",
    "        \"static_categoricals\": statics,\n",
    "        \"targets\": targets,\n",
    "        \"static_reals\": [\"num_neighbors\"]\n",
    "    }\n",
    "\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "\n",
    "    print(f\"Saved to: {filepath}\")"
   ],
   "id": "a9c7de23449ac2c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "save_optimized_dataset(df_final, \"../data/processed/dataset_tft_completo.parquet\")",
   "id": "8946b32bff73ead4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "save_tft_config(known, unknown, statics, targets, \"../data/processed/tft_config.json\")",
   "id": "119b54636011b76c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Graph Embedding",
   "id": "1ca4d05ec4cdd3b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T14:06:31.897239Z",
     "start_time": "2025-11-27T14:06:30.638252Z"
    }
   },
   "cell_type": "code",
   "source": "from src.graph_embedding import generate_graph_embeddings",
   "id": "bc2795ea7d01e23f",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T14:06:32.407571Z",
     "start_time": "2025-11-27T14:06:31.908777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# No seu Jupyter Notebook:\n",
    "df_graph, cols = generate_graph_embeddings(\n",
    "    edges_path=\"../data/processed/adjacencia_edges.csv\",\n",
    "    output_path=\"../data/processed/graph_embeddings.csv\"\n",
    ")"
   ],
   "id": "458a3acc30d93947",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating graph embeddings\n",
      "Building graph\n",
      "Processing 2729 cities\n",
      "Calculating embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\segun\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\dl-master-0LXAJcT3-py3.12\\Lib\\site-packages\\sklearn\\manifold\\_spectral_embedding.py:328: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: ../data/processed/graph_embeddings.csv\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d5e652fb32f16fbc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
